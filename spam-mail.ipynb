{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR--l5DDJOSp",
        "outputId": "b493bde4-ea67-4170-e681-6a98343d8db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           Model  Accuracy  Precision    Recall  F1 Score\n",
            "0  MultinomialNB  0.756522        1.0  0.139932  0.245509\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset into a pandas DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/spam_ham_dataset.csv/spam_ham_dataset.csv')\n",
        "\n",
        "# Clean the dataset by removing escape characters such as \"\\n\" and \"\\r\"\n",
        "df['text'] = df['text'].str.replace('\\n', '').str.replace('\\r', '')\n",
        "\n",
        "# Remove unnecessary columns\n",
        "df.drop(['Unnamed: 0', 'label_num'], axis=1, inplace=True)\n",
        "\n",
        "# Remove stop words from the text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "\n",
        "# Extract features\n",
        "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
        "df['cleaned_text'] = df['text'].apply(lambda x: re.findall(r'\\b[A-Za-z]+\\b', x.lower()))  # Extract only words\n",
        "df['most_appearing_word'] = df['cleaned_text'].apply(lambda x: Counter(x).most_common(1)[0][0])\n",
        "df['capital_letters_count'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
        "df['capital_letter_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / max(1, sum(1 for c in x if c.isalpha())))\n",
        "df['exclamation_mark_count'] = df['text'].str.count('!')\n",
        "df['email_length'] = df['text'].apply(len)\n",
        "df['spam_word_count'] = df['text'].apply(lambda x: sum(word in spam_words_list for word in x.split()))\n",
        "\n",
        "# Sentiment analysis (simplified approach)\n",
        "def sentiment_analysis(text):\n",
        "    if \"good\" in text.lower():\n",
        "        return \"positive\"\n",
        "    elif \"bad\" in text.lower():\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "df['sentiment'] = df['text'].apply(sentiment_analysis)\n",
        "\n",
        "# Feature Engineering - Extracting spam-related words\n",
        "spam_words_list = [\n",
        "    \"100% more\", \"100% free\", \"100% satisfied\", \"Additional income\", \"Be your own boss\",\n",
        "    \"Best price\", \"Big bucks\", \"Billion\", \"Cash bonus\", \"Cents on the dollar\",\n",
        "    # Add more spam-related words here...\n",
        "]\n",
        "\n",
        "spam_words_list = [word.lower() for word in spam_words_list]\n",
        "df['text'] = df['text'].str.lower()\n",
        "df['spam_word_count'] = df['text'].apply(lambda x: sum(word in spam_words_list for word in x.split()))\n",
        "\n",
        "# Select relevant features for modeling\n",
        "features = ['word_count', 'capital_letters_count', 'capital_letter_ratio', 'exclamation_mark_count',\n",
        "            'email_length', 'spam_word_count']\n",
        "\n",
        "# Prepare data for modeling\n",
        "X = df[features]\n",
        "y = df['label']\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_text = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# Merge features and text data\n",
        "X = np.hstack((X.to_numpy(), X_text.toarray()))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize BernoulliNB model\n",
        "clf = BernoulliNB()\n",
        "\n",
        "# Fit the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results_df = pd.DataFrame([['BernoulliNB', accuracy, precision, recall, f1]],\n",
        "                          columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
        "\n",
        "# Display the results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ymXNklpTNOD2"
      },
      "outputs": [],
      "source": [
        "spam_words_list = [\n",
        "    \"100% more\", \"100% free\", \"100% satisfied\", \"Additional income\", \"Be your own boss\",\n",
        "    \"Best price\", \"Big bucks\", \"Billion\", \"Cash bonus\", \"Cents on the dollar\",\n",
        "    # Add more spam-related words here...\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dsidVucMc4z",
        "outputId": "24e050ed-a21b-484b-c86c-01d5f429fa45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
